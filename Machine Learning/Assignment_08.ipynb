{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 08**"
      ],
      "metadata": {
        "id": "pbEdqtFQCcyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. 1:** What exactly is a feature? Give an example to illustrate your point.\n",
        "\n",
        "**Ans 1:** Features are the basic building blocks of datasets. The quality of the features in your dataset has a major impact on the quality of the insights you will gain when you use that dataset for machine learning.\n",
        "\n",
        "Additionally, different business problems within the same industry do not necessarily require the same features, which is why it is important to have a strong understanding of the business goals of your data science project.\n",
        "\n",
        "In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition, classification and regression."
      ],
      "metadata": {
        "id": "0t51CRaYCj-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kDaNbrKOZVP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. 2:** What are the various circumstances in which feature construction is required?\n",
        "\n",
        "**Ans 2:** The features in your data will directly influence the predictive models you use and the results you can achieve. Our results are dependent on many inter-dependent properties. We need great features that describe the structures inherent in your data. Better features means flexibility. The process of generating new variables (features) based on already existing variables is known as feature construction.\n",
        "\n",
        "Feature Construction is a useful process as it can add more information and give more insights of the data we are dealing with. It is done by transforming the numerical features into categorical features which is done while performing Binning. Also, feature construction is done by decomposing variables so that these new variables can be used in various machine learning algorithms such as the creation of Dummy Variables by performing Encoding. Other ways of constructing include deriving features from the pre-existing features and coming up with more meaningful features."
      ],
      "metadata": {
        "id": "dnNjTd_7ZWA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pLUCi6DaZrSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. 3:** Describe how nominal variables are encoded.\n",
        "\n",
        "**Ans 3:** Nominal data is made of discrete values with no numerical relationship between the different categories — mean and median are meaningless. Animal species is one example. For example, pig is not higher than bird and lower than fish. Ordinal or Label Encoding can be used to transform non-numerical labels into numerical labels (or nominal categorical variables). Numerical labels are always between 1 and the number of classes. The labels chosen for the categories have no relationship. So categories that have some ties or are close to each other lose such information after encoding. The first unique value in your column becomes 1, the second becomes 2, the third becomes 3, and so on."
      ],
      "metadata": {
        "id": "Pq85i0s5Zr4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-1PYfZCGZ5I3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. 4:**  Describe how numeric features are converted to categorical features.\n",
        "\n",
        "**Ans 4:** Converting categorical features into numeric features using domain knowledge.For example, we are given a list of countries and say we know the distance to these countries from india then we can replace it with distance from India. So, every country can be represented as its distance from India."
      ],
      "metadata": {
        "id": "NEcSUw9YZ53f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dZIAeW-aaMtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. 5:** Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach?\n",
        "\n",
        "**Ans 5:** Wrapper method measure the \"usefulness\" of features based on the classifier performance.In contrast,the filter methods pick up the intrinstic properties of the features (i.e., the \"relevance\" of the features) measured via univariate statistics instead of cross-validation performance.\n",
        "\n",
        "The wrapper classification algorithum with joint dimensionality reduction and classification can also be used but these method have high computaion cost,lowe discririnatinative power. Moreover,these methods depend on the efficient selection of classifiers for obtaining high accuracy."
      ],
      "metadata": {
        "id": "soussWNwaNeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QFMlQqUZanyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. 6:** When is a feature considered irrelevant? What can be said to quantify it?\n",
        "\n",
        "**Ans 6:** Features are considered relevant if they are either strongly or weakly relevant, and are considered irrelevant otherwise.\n",
        "\n",
        "Irrelevant features can never contribute to prediction accuracy, by definition. Also to quantify it we need to first check the list of features, There are three types of feature selection:\n",
        "\n",
        "- Wrapper methods (forward, backward, and stepwise selection)\n",
        "- Filter methods (ANOVA, Pearson correlation, variance thresholding)\n",
        "- Embedded methods (Lasso, Ridge, Decision Tree).\n",
        "\n",
        "p-value greater than 0.05 means that the feature is insignificant."
      ],
      "metadata": {
        "id": "jmTCk9HTao0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ksK_WqnAbDmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. 7:** When is a function considered redundant? What criteria are used to identify features that could be redundant?\n",
        "\n",
        "**Ans 7:** A feature is considered redundant when it is highly correlated with other features and does not provide any additional information that would improve the performance of the model.\n",
        "\n",
        "1. Correlation: Features that are highly correlated with each other are likely to be redundant. A correlation coefficient above a certain threshold (e.g. 0.7) can be used to identify highly correlated features.\n",
        "\n",
        "2. Variance: A feature with low variance might not provide much information, and it could be considered redundant.\n",
        "\n",
        "3. Linear dependency: Linear dependency can be identified by using techniques like linear discriminant analysis, principal component analysis, or singular value decomposition. If a feature can be represented as a linear combination of other features, it's considered redundant."
      ],
      "metadata": {
        "id": "wo1sH9CsbEpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4pBYhAyMbkwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. 8:** What are the various distance measurements used to determine feature similarity?\n",
        "\n",
        "**Ans 8:** Four of the most commonly used distance measures in machine learning are as follows:\n",
        "\n",
        "- Hamming Distance: Hamming distance calculates the distance between two binary vectors, also referred to as binary strings or bitstrings for short.\n",
        "\n",
        "- Euclidean Distance: Calculates the distance between two real-valued vectors.\n",
        "\n",
        "- Manhattan Distance: Also called the Taxicab distance or the City Block distance, calculates the distance between two real-valued vectors.\n",
        "\n",
        "- Minkowski Distance: Minkowski distance calculates the distance between two real-valued vectors. It is a generalization of the Euclidean and Manhattan distance measures and adds a parameter, called the “order” or “p“, that allows different distance measures to be calculated. "
      ],
      "metadata": {
        "id": "kLmO2NhSbl23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7nKvuyU-b4DP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. 9:** State difference between Euclidean and Manhattan distances?\n",
        "\n",
        "**Ans 9:** Euclidean & Hamming distances are used to measure similarity or dissimilarity between two sequences.Euclidean distance is extensively applied in analysis of convolutaional codes and Trellis codes.\n",
        "\n",
        "Hamming distance is frequently encountered in the analysis of block codes."
      ],
      "metadata": {
        "id": "sHFkwvVKb5D3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pk9V-oMBcZiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. 10:** Distinguish between feature transformation and feature selection.\n",
        "\n",
        "**Ans 10:** Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones.\n",
        "\n",
        "- Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n",
        "\n",
        "- Feature Transformation is a technique by which we can boost our model performance. Feature transformation is a mathematical transformation in which we apply a mathematical formula to a particular column(feature) and transform the values which are useful for our further analysis. It is also known as Feature Engineering, which is creating new features from existing features that may help in improving the model performance. It refers to the family of algorithms that create new features using the existing features. These new features may not have the same interpretation as the original features, but they may have more explanatory power in a different space rather than in the original space. This can also be used for Feature Reduction. It can be done in many ways, by linear combinations of original features or by using non-linear functions. It helps machine learning algorithms to converge faster."
      ],
      "metadata": {
        "id": "oyaeqCf1caM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZLixfujgddwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. 11:** Make brief notes on any two of the following:\n",
        "1. SVD (Standard Variable Diameter Diameter)\n",
        "2. Collection of features using a hybrid approach\n",
        "3. The width of the silhouette\n",
        "4. Receiver operating characteristic curve\n",
        "\n",
        "**Ans 11:** \n",
        "\n",
        "1. SVD (Singular Value Decomposition): SVD is a linear algebra technique that is used to decompose a matrix into its constituent parts. It can be used to reduce the dimensionality of a dataset, by finding the most important features that explain the most variance in the data. SVD can also be used to find latent features in the data and to deal with missing data.\n",
        "\n",
        "2. Collection of features using a hybrid approach: A hybrid approach is a method of combining different feature selection techniques to improve the performance of a model. This can include techniques such as mutual information, correlation, and p-value. By combining different techniques, a hybrid approach can help to overcome the limitations of individual techniques and to improve the interpretability of the model.\n",
        "\n",
        "3. The width of the silhouette: The silhouette is a measure of how similar an object is to its own cluster compared to other clusters. The width of the silhouette is a measure of the similarity of an object to the other objects in its own cluster. A wide silhouette indicates that the object is well-separated from the other objects in its own cluster, whereas a narrow silhouette indicates that the object is similar to the other objects in its own cluster.\n",
        "\n",
        "4. Receiver Operating Characteristic (ROC) curve: A ROC curve is a graphical representation of the performance of a binary classifier system as the discrimination threshold is varied. It is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The area under the ROC curve (AUC) is a measure of the classifier's performance, with a value of 1 indicating a perfect classifier and a value of 0.5 indicating a classifier no better than random guessing. The ROC curve can be used to evaluate the performance of a classifier and to compare different classifiers."
      ],
      "metadata": {
        "id": "ofbXm2rHdec3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hQlxz0Gjd-f-"
      }
    }
  ]
}